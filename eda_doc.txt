[PROJECT TITLE] - Exploratory Data Analysis Report

Date: [Date Report Generated]
Author(s): [Your Name/Team]
Version: [Version Number, e.g., 1.0]

Table of Contents

Introduction

Dataset Overview & Basic Information
2.1. Training Set (train)
2.2. Validation Set (validation)
2.3. Holdout Set (holdout)

Auxiliary Data Quality Checks
3.1. Training Set (train)
3.2. Validation Set (validation)
3.3. Holdout Set (holdout)
3.4. Summary of Data Quality

Metadata Analysis
4.1. Discrete Columns
4.1.1. Target Variable (RCC) Distribution
4.1.2. Common Discrete Columns (e.g., file extension, token bucket)
4.1.3. Specific Discrete Columns (e.g., LOB)
4.2. Continuous Columns
4.2.1. Common Continuous Columns (e.g., number of tokens)
4.3. Datetime Columns
4.3.1. Specific Datetime Columns (e.g., FileModifiedTime)

Text Content Analysis
5.1. Out-of-Vocabulary (OOV) Analysis (vs. train)
5.2. Type-Token Ratio (TTR) Analysis
5.3. N-gram Analysis (if performed)
5.3.1. Top N-grams per Dataset
5.3.2. Top N-grams per Class (Saved Plots)
5.3.3. N-gram Overlap Analysis

Cross-Feature Analysis

Specific Dataset Comparisons (if performed)
7.1. Comparison: train vs validation
7.2. Comparison: train vs holdout
7.3. [Add other pairs as needed]

Summary & Key Findings

1. Introduction

Briefly state the purpose of this Exploratory Data Analysis (EDA).

Identify the datasets analyzed: train, validation, and holdout.

Mention the primary goal (e.g., understanding data characteristics, identifying potential issues, checking for drift between datasets, assessing suitability for modeling).

State the key target variable (RCC).

Note any major assumptions (e.g., text in processed_text column is assumed preprocessed except for lowercasing).

2. Dataset Overview & Basic Information

Provide the following for each dataset.

2.1. Training Set (train)
* Shape: [Number] rows, [Number] columns.
* Missing Values:
* [Table summarizing columns with missing values, their counts, and percentages. Or state "No missing values found."]
| Column Name | Missing Count | Missing Percentage | |-------------|---------------|--------------------| | [Column A] | [Count] | [Percent]% | | [Column B] | [Count] | [Percent]% | | ... | ... | ... |
* Numerical Column Statistics:
* [Paste the table generated by df.describe() for numerical columns. Add brief comments on ranges or notable values if necessary.]
* Target Variable (RCC) Distribution:
* [Paste the table showing RCC counts and proportions.]
* [Comment briefly on class balance/imbalance.]

2.2. Validation Set (validation)
* Shape: [Number] rows, [Number] columns.
* Missing Values: [Table or statement as above]
* Numerical Column Statistics: [Table from describe()]
* Target Variable (RCC) Distribution: [Table and comments]

2.3. Holdout Set (holdout)
* Shape: [Number] rows, [Number] columns.
* Missing Values: [Table or statement as above]
* Numerical Column Statistics: [Table from describe()]
* Target Variable (RCC) Distribution: [Usually N/A for holdout, state if present or absent.]

3. Auxiliary Data Quality Checks

Report the findings from the auxiliary_eda function for each dataset.

3.1. Training Set (train)
* Full Row Duplicates: Count: [Value], Percentage: [Value]%, Count After Drop: [Value]
* Text Column Duplicates (processed_text): Count: [Value], Percentage (non-null): [Value]%, Unique Count: [Value]
* Conflicting Targets for Duplicate Text: [Report findings - e.g., "Found X texts with conflicting labels. Examples: ...", or "No conflicting labels found."]
* All-NaN Columns: [List columns or state "None found."]
* Zero Variance Columns: [List columns or state "None found."]
* High Cardinality Discrete Columns (> [Threshold]): [List columns and counts or state "None found."]
* Memory Usage: [Value] KB/MB/GB

3.2. Validation Set (validation)
* Full Row Duplicates: Count: [Value], Percentage: [Value]%, Count After Drop: [Value]
* Text Column Duplicates (processed_text): Count: [Value], Percentage (non-null): [Value]%, Unique Count: [Value]
* Conflicting Targets for Duplicate Text: [Report findings]
* All-NaN Columns: [List columns or state "None found."]
* Zero Variance Columns: [List columns or state "None found."]
* High Cardinality Discrete Columns (> [Threshold]): [List columns and counts or state "None found."]
* Memory Usage: [Value] KB/MB/GB

3.3. Holdout Set (holdout)
* Full Row Duplicates: Count: [Value], Percentage: [Value]%, Count After Drop: [Value]
* Text Column Duplicates (processed_text): Count: [Value], Percentage (non-null): [Value]%, Unique Count: [Value]
* Conflicting Targets for Duplicate Text: [Usually N/A, state "Skipped as target column is absent."]
* All-NaN Columns: [List columns or state "None found."]
* Zero Variance Columns: [List columns or state "None found."]
* High Cardinality Discrete Columns (> [Threshold]): [List columns and counts or state "None found."]
* Memory Usage: [Value] KB/MB/GB

3.4. Summary of Data Quality
* [Provide a brief overall summary based on the checks. Are there significant duplicates? Any problematic columns (all NaN, zero variance)? Any immediate data cleaning needs identified?]

4. Metadata Analysis

4.1. Discrete Columns

**4.1.1. Target Variable (`RCC`) Distribution**
    *   [Insert Plot: Comparison Bar Chart of `RCC` proportions across `train`, `validation`, `holdout`.]
    *   [Comment on the consistency of the target distribution across the labeled datasets. Is the validation/holdout split representative?]

**4.1.2. Common Discrete Columns**
    *   **`file extension`:**
        *   [Insert Plot: Comparison Bar Chart of `file extension` proportions across `train`, `validation`, `holdout`.]
        *   [Comment on distribution consistency or shifts.]
    *   **`token bucket`:**
        *   [Insert Plot: Comparison Bar Chart of `token bucket` proportions across `train`, `validation`, `holdout` (using the defined order).]
        *   [Comment on distribution consistency or shifts.]
    *   [Add sections for any other common discrete columns.]

**4.1.3. Specific Discrete Columns**
    *   **`LOB`:**
        *   [Insert Plot: Comparison Bar Chart of `LOB` proportions across datasets where it exists (likely `holdout` only, or maybe `validation` if sampled from `holdout`). Adjust comparison description accordingly.]
        *   [Comment on the distribution.]
    *   [Add sections for any other specific discrete columns.]


4.2. Continuous Columns

**4.2.1. Common Continuous Columns**
    *   **`number of tokens`:**
        *   [Insert Plot: Comparison Box Plot (Outliers Hidden) of `number of tokens` across `train`, `validation`, `holdout`.]
        *   [Comment on differences/similarities in median, IQR, and overall range across datasets.]
        *   [Insert Plot: Box Plot (Outliers Hidden) of `number of tokens` by `RCC` for `train` dataset.]
        *   [Insert Plot: Box Plot (Outliers Hidden) of `number of tokens` by `RCC` for `validation` dataset.]
        *   [Insert Plot: Box Plot (Outliers Hidden) of `number of tokens` by `RCC` for `holdout` dataset (if `RCC` is present).]
        *   [Comment on whether token count appears to differ significantly between classes within each dataset. Note if log scale was applied.]
    *   [Add sections for any other common continuous columns.]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

4.3. Datetime Columns

**4.3.1. Specific Datetime Columns**
    *   **`FileModifiedTime`:**
        *   [Insert Plot: Bar Chart of Document Count per Year/Bucket for `train` (if present).]
        *   [Insert Plot: Bar Chart of Document Count per Year/Bucket for `validation` (if present).]
        *   [Insert Plot: Bar Chart of Document Count per Year/Bucket for `holdout` (if present).]
        *   [Comment on the temporal distribution within each dataset. Are there peaks/trends? Note if bucketing was applied.]
        *   *(If specific comparison plot was generated in Section 8 for this column, refer to it here or embed it again)*
    *   [Add sections for any other specific datetime columns.]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

5. Text Content Analysis

(This section is generated only if analyze_text_content=True)

5.1. Out-of-Vocabulary (OOV) Analysis (vs. train)
* [Insert Plot: Bar Chart of Token-Based OOV % for validation, holdout.]
* [Insert Plot: Bar Chart of Unique Word-Based OOV % for validation, holdout.]
* [Discuss the OOV levels. Are they high or low? Is there a significant difference between validation and holdout? What does this imply about vocabulary drift?]

5.2. Type-Token Ratio (TTR) Analysis
* [Insert Plot: Bar Chart comparing TTR % (Stopwords Removed) per Class across train, validation, holdout.]
* [Discuss the TTR findings. Which classes/datasets show higher/lower lexical diversity? Are the trends consistent across datasets? Remember the caveat about text length.]

5.3. N-gram Analysis (if analyze_ngrams=True)

**5.3.1. Top N-grams per Dataset**
    *   [Insert Plot: Top N Unigrams for `train` dataset.]
    *   [Insert Plot: Top N Bigrams for `train` dataset (if `analyze_bigrams=True`).]
    *   [Insert Plot: Top N Unigrams for `validation` dataset.]
    *   [Insert Plot: Top N Bigrams for `validation` dataset (if `analyze_bigrams=True`).]
    *   [Insert Plot: Top N Unigrams for `holdout` dataset.]
    *   [Insert Plot: Top N Bigrams for `holdout` dataset (if `analyze_bigrams=True`).]
    *   [Comment on the most frequent terms/phrases in each dataset. Are there notable differences suggesting different content focus or drift?]
    *   [Mention the calculated Overall N-gram overlap percentages between train/validation and train/holdout.]

**5.3.2. Top N-grams per Class (Saved Plots)**
    *   Plots for the top [N] unigrams (and bigrams, if enabled) for each `RCC` class within the `train`, `validation`, and `holdout` datasets have been generated and saved to: `[Specify base_save_path]/ngram_analysis/ngrams_per_class/`.
    *   [Optionally, select 1-2 interesting class examples and embed their plots here, discussing key terms.]

**5.3.3. N-gram Overlap Analysis**
    *   [Paste the DataFrame table showing the per-class unigram (and bigram) overlap percentages between `train` vs `validation` and `train` vs `holdout`.]
    *   [Discuss the overlap findings. Which classes show high/low consistency in their top terms between datasets? Does this align with expectations?]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

6. Cross-Feature Analysis

[Insert Plot: Example 1 Plot (e.g., file extension vs number of tokens).]

[Comment on the observed relationship across datasets.]

[Insert Plot: Example 2 Plot (e.g., file extension vs RCC stacked bar) for train.]

[Insert Plot: Example 2 Plot for validation.]

[Insert Plot: Example 2 Plot for holdout (if applicable).]

[Comment on the observed relationship and its consistency across datasets.]

[Include any other relevant cross-feature plots generated.]

7. Specific Dataset Comparisons

(This section is generated only if specific_comparisons was provided)

7.1. Comparison: train vs validation
* [Summarize key findings from the direct comparison plots generated in Section 8 for this pair.]
* [Focus on similarities and differences identified in common columns (discrete proportions, continuous distributions, datetime trends).]
* [Embed or reference the most relevant comparison plots from Section 8, e.g., the datetime comparison plot.]

7.2. Comparison: train vs holdout
* [Summarize key findings from the direct comparison plots generated in Section 8 for this pair.]
* [Focus on similarities and differences, especially potential drift.]
* [Embed or reference key comparison plots.]

7.3. [Add other pairs as specified in specific_comparisons]
* [Repeat summary and reference structure for other pairs.]

8. Summary & Key Findings

Data Quality: Briefly summarize the overall data quality based on duplicates, missing values, constant columns etc.

Distributions & Consistency: Summarize the key observations about metadata and target distributions. Are validation and holdout distributions similar to train? Where are the main differences?

Vocabulary & Content: Summarize OOV, TTR, and N-gram findings. Is there significant vocabulary drift? Do classes have distinct language?

Potential Issues: Explicitly list any potential problems identified (e.g., high OOV in holdout, class imbalance, conflicting labels, specific data quality concerns).

Recommendations: Suggest potential next steps based on the EDA (e.g., specific data cleaning, preprocessing considerations like handling high cardinality features, awareness of potential drift for model evaluation, features that might be discriminative).
